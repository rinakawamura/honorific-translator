{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook to Train Neural Translation Models from Scratch using OpenNMT\n",
        "By Rina Kawamura"
      ],
      "metadata": {
        "id": "9WvIqQtRHq2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary package for tokenization\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "ERBbCcsFHhv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing\n",
        "# Split train, dev, test files into separate honorific and regular sentence files\n",
        "import json\n",
        "\n",
        "# Input path to train data (JSON file)\n",
        "train_f = \n",
        "train_json = open(train_f, 'r')\n",
        "data_dict = json.load(train_json)\n",
        "train_json.close()\n",
        "\n",
        "# Specify paths for regular and honorific files for train data\n",
        "with open([Path to Honorific Data], 'w') as hon_f, open([Path to Regular Data], 'w') as reg_f:\n",
        "  for pair in data_dict[\"data\"]:\n",
        "    hon_f.write(pair[\"hon\"] + '\\n')\n",
        "    reg_f.write(pair[\"reg\"] + '\\n')\n",
        "  \n",
        "# Input path to validation data (JSON file)\n",
        "dev_f = \"data/dev.json\"\n",
        "dev_json = open(dev_f, 'r')\n",
        "data_dict = json.load(dev_json)\n",
        "dev_json.close()\n",
        "\n",
        "# Specify paths for regular and honorific files for validation data\n",
        "with open([Path to Honorific Data], 'w') as hon_f, open([Path to Regular Data], 'w') as reg_f:\n",
        "  for pair in data_dict[\"data\"]:\n",
        "    hon_f.write(pair[\"hon\"] + '\\n')\n",
        "    reg_f.write(pair[\"reg\"] + '\\n')\n",
        "\n",
        "# Input path to test data (JSON file)\n",
        "test_f = \"data/test.json\"\n",
        "test_json = open(test_f, 'r')\n",
        "data_dict = json.load(test_json)\n",
        "test_json.close()\n",
        "\n",
        "# Specify paths for regular and honorific files for test data\n",
        "with open([Path to Honorific Data], 'w') as hon_f, open([Path to Regular Data], 'w') as reg_f:\n",
        "  for pair in data_dict[\"data\"]:\n",
        "    hon_f.write(pair[\"hon\"] + '\\n')\n",
        "    reg_f.write(pair[\"reg\"] + '\\n')"
      ],
      "metadata": {
        "id": "HoD7kmj-MlQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab to train on by combining all training data into one file\n",
        "# Specify train honorific and regular files, as well as file to put all training sentences in\n",
        "!cat [Path to Train Honorific] [Path to Train Regular] > [Path to Train All]\n",
        "\n",
        "# Subword Tokenization\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Train sentencepiece tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "spm.SentencePieceTrainer.Train(\"--input=data/basic/train.all --model_prefix=ja --vocab_size=16000 --character_coverage=0.9995\")"
      ],
      "metadata": {
        "id": "ccUJO3SwHkJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize all data\n",
        "# Specify path to trained tokenizer\n",
        "sp.load([Path to Tokenizer])\n",
        "def detok(filename, outputfilename):\n",
        "    f = open(filename, 'r')\n",
        "    fout = open(outputfilename, 'w')\n",
        "    content = f.read()\n",
        "    lines = content.split('\\n')\n",
        "    for line in lines:\n",
        "        data = sp.EncodeAsPieces(str(line))\n",
        "        data = ' '.join(data)\n",
        "        fout.write(data + '\\n')\n",
        "    f.close()\n",
        "    fout.close()\n",
        "\n",
        "# Specify paths for tokenized data\n",
        "detok([Path to Honorific Train Data],[Path to Tokenized Honorific Train Data])\n",
        "detok([Path to Regular Train Data],[Path to Tokenized Regular Train Data])\n",
        "detok([Path to Honorific Validation Data],[Path to Tokenized Honorific Validation Data])\n",
        "detok([Path to Regular Validation Data],[Path to Tokenized Regular Validation Data])\n",
        "detok([Path to Honorific Test Data],[Path to Tokenized Honorific Test Data])\n",
        "detok([Path to Regular Test Data],[Path to Tokenized Regular Test Data])"
      ],
      "metadata": {
        "id": "JMrMacE-VQxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use OpenNMT library to train translation model\n",
        "# Set up OpenNMT\n",
        "!git clone -b legacy https://github.com/OpenNMT/OpenNMT-py\n",
        "\n",
        "# Change to OpenNMT directory\n",
        "%cd OpenNMT-py\n",
        "!pip install -r requirements.opt.txt\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "ufA7BQZVXEgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data for training\n",
        "# Specify paths\n",
        "!python preprocess.py -train_src [Path to Tokenized Regular Train Data] -train_tgt [Path to Tokenized Honorific Train Data] -valid_src [Path to Tokenized Regular Validation Data] -valid_tgt [Path to Tokenized Honorific Validation Data] -save_data [Path to Save Preprocessed Data]"
      ],
      "metadata": {
        "id": "mLg6u761XrSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup GPU (Change as necessary)\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "# Train transformer model \n",
        "# Specify paths to preprocessed data and where to save model (Make other alterations where necessary)\n",
        "!python train.py -data [Path to Preprocessed Data] -save_model [Path to Save Model] \\\n",
        "-layers 6 -rnn_size 512 -word_vec_size 512 \\\n",
        "-transformer_ff 2048 -heads 8 -encoder_type transformer -decoder_type transformer -position_encoding \\\n",
        "-train_steps 5000 -early_stopping 3 -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens \\\n",
        "-normalization tokens -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam \\\n",
        "-warmup_steps 4000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot \\\n",
        "-label_smoothing 0.1 -valid_steps 500 -save_checkpoint_steps 500 -world_size 1 -gpu_ranks 0 "
      ],
      "metadata": {
        "id": "XSMKXs1zebwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use transformer model to translate test set\n",
        "# Specify paths to model, tokenized test data, and save output\n",
        "!python translate.py -model [Path to Trained Model] -src [Path to Tokenized Regular Test Data] -output [Path to Save Tokenized Translated Sentences] -replace_unk -verbose -gpu 0"
      ],
      "metadata": {
        "id": "WeeYQIDbq6yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train basic OpenNMT default model (LSTM)\n",
        "# Specify paths to preprocessed data and where to save model (Make other alterations where necessary)\n",
        "!python train.py -data [Path to Preprocessed Data] -save_model [Path to Save Model] -world_size 1 -gpu_ranks 0 # -train_from [Specify Checkpoint If Any]"
      ],
      "metadata": {
        "id": "nWGgzaYPKw_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate sentences with basic (LSTM) model\n",
        "# Specify paths to model, tokenized test data, and save output\n",
        "!python translate.py -model [Path to Trained Model] -src [Path to Tokenized Regular Test Data] -output [Path to Save Tokenized Translated Sentences] -replace_unk -verbose -gpu 0"
      ],
      "metadata": {
        "id": "MXitA-8YkdSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detokenize translated text\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# Specify path to tokenizer\n",
        "sp.load([Path to Tokenizer (Trained Above)])\n",
        "\n",
        "# Transformer Model\n",
        "# Path to tokenized transformer translated sentences\n",
        "file_name = \n",
        "f = open(file_name, 'r')\n",
        "data = f.read()\n",
        "sents = data.split('\\n')\n",
        "f.close()\n",
        "\n",
        "# Specify path to save detokenized sentences\n",
        "out_f = open([Path to Detokenized Transformer Sentences], 'w')\n",
        "for sent in sents:\n",
        "    toks = sent.split(' ')\n",
        "    tmp = sp.decode_pieces(toks)\n",
        "    out_f.write(tmp + '\\n')\n",
        "out_f.close()\n",
        "\n",
        "# Basic (LSTM) Model\n",
        "# Path to tokenized LSTM translated sentences\n",
        "file_name = \n",
        "f = open(file_name, 'r')\n",
        "data = f.read()\n",
        "sents = data.split('\\n')\n",
        "f.close()\n",
        "\n",
        "# Specify path to save detokenized sentences\n",
        "out_f = open([Path to Detokenized LSTM Sentences], 'w')\n",
        "for sent in sents:\n",
        "    toks = sent.split(' ')\n",
        "    tmp = sp.decode_pieces(toks)\n",
        "    out_f.write(tmp + '\\n')\n",
        "out_f.close()"
      ],
      "metadata": {
        "id": "WmnKCk4Nr1yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages to calculate BLEU score\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ],
      "metadata": {
        "id": "lv5IDoQIjXRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse using Mecab\n",
        "import MeCab\n",
        "\n",
        "# Parse translated and reference sentences using Mecab\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "# Get reference honorific sentences from file\n",
        "# Specify path to reference honorific sentences\n",
        "ref_file_name = \n",
        "ref_f = open(ref_file_name, 'r')\n",
        "ref_data = ref_f.read()\n",
        "ref_sents = ref_data.split('\\n')\n",
        "ref_f.close()\n",
        "\n",
        "# Get transformer translated honorific sentences from file\n",
        "# Specify path to Transformer-translated honorific sentences\n",
        "tr_file_name = \n",
        "tr_f = open(tr_file_name, 'r')\n",
        "tr_data = tr_f.read()\n",
        "tr_sents = tr_data.split('\\n')\n",
        "tr_f.close()\n",
        "\n",
        "# Get LSTM translated honorific sentences from file\n",
        "# Specify path to LSTM-translated honorific sentences\n",
        "basic_file_name = \n",
        "basic_f = open(basic_file_name, 'r')\n",
        "basic_data = basic_f.read()\n",
        "basic_sents = basic_data.split('\\n')\n",
        "basic_f.close()\n",
        "\n",
        "# Write Mecab parsed sentences to files\n",
        "with open([Path to Mecab-parsed Transfomer Sentences], 'w') as tr_out, open([Path to Mecab-parsed LSTM Sentences], 'w') as basic_out, open([Path to Mecab-parsed Reference Sentences], 'w') as ref_out:\n",
        "  for i in range(len(tr_sents)):\n",
        "    tr_out.write(wakati.parse(tr_sents[i]))\n",
        "    basic_out.write(wakati.parse(basic_sents[i]))\n",
        "    ref_out.write(wakati.parse(ref_sents[i]))\n"
      ],
      "metadata": {
        "id": "5sC8w2osjZss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation using BLEU\n",
        "\n",
        "# Use OpenNMT BLEU Scorer Tool\n",
        "# Transformer Score\n",
        "!perl tools/multi-bleu.perl [Path to Mecab-parsed Reference Sentences] < [Path to Mecab-parsed Transformer Sentences]\n",
        "# LSTM Score\n",
        "!perl tools/multi-bleu.perl [Path to Mecab-parsed Reference Sentences] < [Path to Mecab-parsed LSTM Sentences]"
      ],
      "metadata": {
        "id": "5cMu32UQoVE6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}