{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Pretrained Japanese GPT-II Model for Honorific Translation\n",
        "By Rina Kawamura"
      ],
      "metadata": {
        "id": "0ckzTUdcH4qN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzaQtebQYxpN"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV297hxMofjF"
      },
      "outputs": [],
      "source": [
        "# Data processing (Split, Process, Tokenize)\n",
        "import json\n",
        "\n",
        "# Add paths for processed training and validation data to be saved (txt file)\n",
        "train_path = \n",
        "dev_path = \n",
        "\n",
        "# Add path to train data (JSON file)\n",
        "train_file = \n",
        "\n",
        "train_json = open(train_file, 'r')\n",
        "train_data = json.load(train_json)\n",
        "train_json.close()\n",
        "\n",
        "# Add path to validation data (JSON file)\n",
        "dev_file = \n",
        "\n",
        "dev_json = open(dev_file, 'r')\n",
        "dev_data = json.load(dev_json)\n",
        "dev_json.close()\n",
        "\n",
        "# Put training data in format of [Start Token (<s>)][Regular Sentence][Separation Token ([SEP])][Honorific Sentence][End Token (</s>)]\n",
        "\n",
        "with open(train_path, 'w') as f:\n",
        "  bos_tok, sep_tok, eos_tok = '<s>', '[SEP]', '</s>'\n",
        "  for pair in train_data[\"data\"]:\n",
        "    full = bos_tok + pair['reg'] + sep_tok + pair['hon'] + eos_tok + '\\n'\n",
        "    f.write(full)\n",
        "\n",
        "with open(dev_path, 'w') as f:\n",
        "  bos_tok, sep_tok, eos_tok = '<s>', '[SEP]', '</s>'\n",
        "  for pair in dev_data[\"data\"]:\n",
        "    full = bos_tok + pair['reg'] + sep_tok + pair['hon'] + eos_tok + '\\n'\n",
        "    f.write(full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQVagasOrtxy"
      },
      "outputs": [],
      "source": [
        "# Load necessary tokenizer and model\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Load Japanese gpt2 model tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "tokenizer.do_lower_case = True\n",
        "\n",
        "# Load pretrained model\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm8eBe0FnXtr"
      },
      "outputs": [],
      "source": [
        "# Finetune GPT-II model\n",
        "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
        "from transformers import TrainingArguments, Trainer, DataCollator, TextDataset\n",
        "\n",
        "train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "eval_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=dev_path,\n",
        "          block_size=128)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Specify directory to save checkpoints\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2/\", \n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Specify Trainer model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint = True)\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV8c-vUNEoCM"
      },
      "outputs": [],
      "source": [
        "# Load finetuned model for inference\n",
        "from transformers import pipeline, GPT2LMHeadModel\n",
        "\n",
        "# Specify path to saved model and config file\n",
        "my_model = GPT2LMHeadModel.from_pretrained([Path to Model File],config=[Path to Config File])\n",
        "translator = pipeline('text-generation',model=my_model, tokenizer='rinna/japanese-gpt2-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnKn15MRMume"
      },
      "outputs": [],
      "source": [
        "# Get regular test sentences to convert using model\n",
        "# Write reference sentences to file to compare later\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify path to test data (JSON file)\n",
        "test_file = \n",
        "\n",
        "test_json = open(test_file, 'r')\n",
        "test_data = json.load(test_json)\n",
        "test_json.close()\n",
        "\n",
        "# Specify paths to save regular and honorific test sentences\n",
        "test_ref_path = \n",
        "test_reg_path = \n",
        "\n",
        "test_reg_sents = []\n",
        "with open(test_ref_path, 'w') as ref_f, open(test_reg_path, 'w') as reg_f:\n",
        "  for pair in test_data[\"data\"]:\n",
        "    test_reg_sents.append(pair['reg'])\n",
        "    ref_f.write(pair['hon'] + '\\n')\n",
        "    reg_f.write(pair['reg'] + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfN3PPsrGU5m"
      },
      "outputs": [],
      "source": [
        "# Translate test data\n",
        "def translate_w_model(sent):\n",
        "  bos_tok, sep_tok, eos_tok = '<s>', '[SEP]', '</s>'\n",
        "  input = bos_tok + sent + sep_tok\n",
        "  return (translator(input))\n",
        "\n",
        "# Remove special tokens from translated sentences\n",
        "def process_translated(obj):\n",
        "  toks = obj[0]['generated_text'].split('[SEP]')\n",
        "  return toks[-1]\n",
        "\n",
        "# Specify path to save translated honorific sentences\n",
        "test_out_path = \n",
        "with open(test_out_path, 'a') as out_f:\n",
        "  for sent in test_reg_sents:\n",
        "    tr = translate_w_model(sent)\n",
        "    proc = process_translated(tr)\n",
        "    out_f.write(proc + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25alpGJTfbIa"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages to calculate BLEU score\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48saBbtDszqn"
      },
      "outputs": [],
      "source": [
        "# Evaluate using BLEU\n",
        "import MeCab\n",
        "\n",
        "# Parse translated and reference sentences using Mecab\n",
        "wakati = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "# Get translated honorific sentences from file\n",
        "# Specify path to translated sentences\n",
        "tr_f = open([Path to Translated Sentences], 'r')\n",
        "tr_data = tr_f.read()\n",
        "tr_sents = tr_data.split('\\n')\n",
        "tr_f.close()\n",
        "\n",
        "# Get reference honorific sentences from file\n",
        "# Specify path to reference honorific sentences\n",
        "ref_f = open([Path to References Sentences], 'r')\n",
        "ref_data = ref_f.read()\n",
        "ref_sents = ref_data.split('\\n')\n",
        "ref_f.close()\n",
        "\n",
        "# Specify paths to write MeCab-parsed outputs for the translated and reference sentences\n",
        "with open([Path to Parsed Translated Sentences], 'w') as tr_out, open([Path to Parsed Reference Sentences], 'w') as ref_out:\n",
        "  for i in range(len(tr_sents)):\n",
        "    tr_out.write(wakati.parse(tr_sents[i]))\n",
        "    ref_out.write(wakati.parse(ref_sents[i]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl7QhUskgk2T"
      },
      "outputs": [],
      "source": [
        "# Use OpenNMT BLEU Scorer Tool\n",
        "# Input paths to Mecab-parsed reference and translated sentence files\n",
        "!perl  OpenNMT-py/tools/multi-bleu.perl [Path to Parsed References Sentences] < [Path to Parsed Translated Sentences]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "finetune.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}