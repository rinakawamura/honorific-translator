{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "use_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Use Trained Models (LSTM, Transformer, Finetuned GPT-II)\n",
        "By Rina Kawamura"
      ],
      "metadata": {
        "id": "9_i4qyzNILeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup to use OpenNMT Models\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Specify path to cloned OpenNMT directory\n",
        "%cd [Path to OpenNMT Directory]\n",
        "!pip install -r requirements.opt.txt\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "OzlrHE8Zu9jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize test sentences\n",
        "sp = spm.SentencePieceProcessor()\n",
        "# Specify path to trained tokenizer\n",
        "sp.load([Path to Trained Tokenizer])\n",
        "\n",
        "def detok(filename, outputfilename):\n",
        "    f = open(filename, 'r')\n",
        "    fout = open(outputfilename, 'w')\n",
        "    content = f.read()\n",
        "    lines = content.split('\\n')\n",
        "    for line in lines:\n",
        "        data = sp.EncodeAsPieces(str(line))\n",
        "        data = ' '.join(data)\n",
        "        fout.write(data + '\\n')\n",
        "    f.close()\n",
        "    fout.close()\n",
        "\n",
        "# Tokenize regular sentences to translate\n",
        "detok([Path to Regular Sentences],[Path to Save Tokenized Regular Sentences])"
      ],
      "metadata": {
        "id": "N8sWveCVvrq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use transformer model to translate human evaluation test set\n",
        "# Specify GPU\n",
        "!export CUDA_VISIBLE_DEVICES=0\n",
        "# Specify path to transformer model, path to tokenized regular sentences, and path to output tokenized translated sentences\n",
        "!python translate.py -model [Path to Trained Transformer Model] -src [Path to Tokenized Regular Sentences] -output [Path to Save Tokenized Translated Sentences] -replace_unk -verbose -gpu 0"
      ],
      "metadata": {
        "id": "xD5teKQxvMSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use LSTM model to translate\n",
        "# Specify path to transformer model, path to tokenized regular sentences, and path to output tokenized translated sentences\n",
        "!python translate.py -model [Path to Trained LSTM Model] -src [Path to Tokenized Regular Sentences] -output [Path to Save Tokenized Translated Sentences] -replace_unk -verbose -gpu 0"
      ],
      "metadata": {
        "id": "HLpfhvLlwjEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detokenize translated tokens\n",
        "# Transformer Model\n",
        "# Specify path to tokenized translated sentences\n",
        "file_name = \n",
        "f = open(file_name, 'r')\n",
        "data = f.read()\n",
        "sents = data.split('\\n')\n",
        "f.close()\n",
        "\n",
        "# Specify path to save detokenized translated sentences\n",
        "out_f = open([Path to Save Transformer-translated Sentences], 'w')\n",
        "for sent in sents:\n",
        "    toks = sent.split(' ')\n",
        "    tmp = sp.decode_pieces(toks)\n",
        "    out_f.write(tmp + '\\n')\n",
        "out_f.close()\n",
        "\n",
        "# LSTM Model\n",
        "# Specify path to tokenized translated sentences\n",
        "file_name = \n",
        "f = open(file_name, 'r')\n",
        "data = f.read()\n",
        "sents = data.split('\\n')\n",
        "f.close()\n",
        "\n",
        "# Specify path to save detokenized translated sentences\n",
        "out_f = open([Path to Save LSTM-translated Sentences], 'w')\n",
        "for sent in sents:\n",
        "    toks = sent.split(' ')\n",
        "    tmp = sp.decode_pieces(toks)\n",
        "    out_f.write(tmp + '\\n')\n",
        "out_f.close()"
      ],
      "metadata": {
        "id": "Uz9Q74RvwcdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave OpenNMT Directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "KjyurgPdyXuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup to use finetuned GPT-II model\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "\n",
        "# Load necessary tokenizer and model\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Load Japanese gpt2 model tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "tokenizer.do_lower_case = True\n",
        "\n",
        "# Load pretrained model\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-small\")"
      ],
      "metadata": {
        "id": "ap-zjsOlyeyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, GPT2LMHeadModel\n",
        "\n",
        "# Specify path to trained model\n",
        "my_model = GPT2LMHeadModel.from_pretrained([Path to Finetuned Model],config=[Path to Config File])\n",
        "translator = pipeline('text-generation',model=my_model, tokenizer='rinna/japanese-gpt2-small')"
      ],
      "metadata": {
        "id": "VbIQ_DQ7yozC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_sents = []\n",
        "# Path to regular sentences to translated\n",
        "test_file = open([Path to Regular Sentences], 'r')\n",
        "test_data = test_file.readlines()\n",
        "test_file.close()\n",
        "\n",
        "for line in test_data:\n",
        "  reg_sents.append(line.strip())"
      ],
      "metadata": {
        "id": "IkVD4YPdy_Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate test data\n",
        "def translate_w_model(sent):\n",
        "  bos_tok, sep_tok, eos_tok = '<s>', '[SEP]', '</s>'\n",
        "  input = bos_tok + sent + sep_tok\n",
        "  return (translator(input))\n",
        "\n",
        "# Process translated data\n",
        "def process_translated(obj):\n",
        "  toks = obj[0]['generated_text'].split('[SEP]')\n",
        "  return toks[-1]\n",
        "\n",
        "# Specify path to save translated sentences\n",
        "test_out_path = \n",
        "with open(test_out_path, 'a') as out_f:\n",
        "  for sent in reg_sents:\n",
        "    tr = translate_w_model(sent)\n",
        "    proc = process_translated(tr)\n",
        "    out_f.write(proc + '\\n')"
      ],
      "metadata": {
        "id": "ahSlm6akywq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}